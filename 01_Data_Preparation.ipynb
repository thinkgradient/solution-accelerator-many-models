{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "---\n",
    "In this notebook we will use a dummy sales forecasting dataset that has some made up historical sales figures by skus for each week going back to a couple of years worth of data. The original many models solution accelerator repo uses the simulated orange juice sales data. To use that please refere to the original many models github repo: https://github.com/microsoft/solution-accelerator-many-models\n",
    "\n",
    "The dataset will be very simple - 3 columns only: Week | Sku01 | Sku02 | Sku03 .... and so on. For the many models solution accelerator to work with this dataset, we must generate a file for each Sku so that we run a model per sku. Each file will subsequently have the following format: Week | Sku | Sales\n",
    "\n",
    "### Prerequisites\n",
    "If you have already run the [00_Setup_AML_Workspace](00_Setup_AML_Workspace.ipynb) notebook you are all set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.append('scripts')\n",
    "from helper import process_denormalized_skus\n",
    "from helper import split_data"
   ]
  },
  {
   "source": [
    "data_folder = 'data'\n",
    "processed_folder = 'processed_data'\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "os.makedirs(data_folder + \"/\" + processed_folder, exist_ok=True)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalized_skus = 'historical_sales.csv'\n",
    "file_extension = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_col = 'Week'\n",
    "split_date = '18/05/2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_created = process_denormalized_skus(timestamp_col, data_folder, processed_folder, denormalized_skus, file_extension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'files_created': 5}\n"
     ]
    }
   ],
   "source": [
    "print(files_created)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Split data in two sets\n",
    "\n",
    "We will now split each dataset in two parts: one will be used for training, and the other will be used for simulating batch forecasting. The training files will contain the data records before '1992-5-28' and the last part of each series will be stored in the inferencing files.\n",
    "\n",
    "Finally, we will upload both sets of data files to the Workspace's default [Datastore](https://docs.microsoft.compython/api/azureml-core/azureml.core.datastore(class))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = data_folder + \"/\" + processed_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, inference_path = split_data(target_path, timestamp_col, split_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Upload data to Datastore in AML Workspace\n",
    "\n",
    "In the [setup notebook](00_Setup_AML_Workspace.ipynb) you created a [Workspace](https://docs.microsoft.com/python/api/azureml-core/azureml.core.workspace.workspace). We are going to register the data in that enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING - Warning: Falling back to use azure cli login credentials.\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\nPlease refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/051aa254-957d-4431-a6df-6caa8963bdd7/resourceGroups/manymodelsfatos/providers/Microsoft.MachineLearningServices/workspaces/mmaml',\n",
       " 'name': 'mmaml',\n",
       " 'location': 'westeurope',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'sku': 'Enterprise',\n",
       " 'workspaceid': '64638bba-4983-4100-9dd1-f411313ac67a',\n",
       " 'description': '',\n",
       " 'friendlyName': 'mmaml',\n",
       " 'creationTime': '2020-09-15T13:55:07.5489274+00:00',\n",
       " 'containerRegistry': '/subscriptions/051aa254-957d-4431-a6df-6caa8963bdd7/resourceGroups/manymodelsfatos/providers/Microsoft.ContainerRegistry/registries/64638bba498341009dd1f411313ac67a',\n",
       " 'keyVault': '/subscriptions/051aa254-957d-4431-a6df-6caa8963bdd7/resourcegroups/manymodelsfatos/providers/microsoft.keyvault/vaults/kvxwcqolhnxvyp6',\n",
       " 'applicationInsights': '/subscriptions/051aa254-957d-4431-a6df-6caa8963bdd7/resourcegroups/manymodelsfatos/providers/microsoft.insights/components/aixwcqolhnxvyp6',\n",
       " 'identityPrincipalId': '612c3456-be85-408a-9c77-e9cd62a730c5',\n",
       " 'identityTenantId': '72f988bf-86f1-41af-91ab-2d7cd011db47',\n",
       " 'identityType': 'SystemAssigned',\n",
       " 'storageAccount': '/subscriptions/051aa254-957d-4431-a6df-6caa8963bdd7/resourcegroups/manymodelsfatos/providers/microsoft.storage/storageaccounts/saxwcqolhnxvyp6',\n",
       " 'hbiWorkspace': False,\n",
       " 'discoveryUrl': 'https://westeurope.experiments.azureml.net/discovery',\n",
       " 'notebookInfo': {'fqdn': 'ml-mmaml-westeurope-64638bba-4983-4100-9dd1-f411313ac67a.notebooks.azure.net',\n",
       "  'resource_id': '7ab0f78318594210beb2c9d1cd88e309'}}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Take a look at Workspace\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'data/processed_data'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "target_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will upload both sets of data files to your Workspace's default [Datastore](https://docs.microsoft.com/azure/machine-learning/how-to-access-data). \n",
    "A Datastore is a place where data can be stored that is then made accessible for training or forecasting. Please refer to [Datastore documentation](https://docs.microsoft.com/python/api/azureml-core/azureml.core.datastore(class)) on how to access data from Datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Uploading an estimated of 5 files\n",
      "Uploading data/processed_data\\upload_train_data\\sku01.csv\n",
      "Uploaded data/processed_data\\upload_train_data\\sku01.csv, 1 files out of an estimated total of 5\n",
      "Uploading data/processed_data\\upload_train_data\\sku02.csv\n",
      "Uploaded data/processed_data\\upload_train_data\\sku02.csv, 2 files out of an estimated total of 5\n",
      "Uploading data/processed_data\\upload_train_data\\sku03.csv\n",
      "Uploaded data/processed_data\\upload_train_data\\sku03.csv, 3 files out of an estimated total of 5\n",
      "Uploading data/processed_data\\upload_train_data\\sku04.csv\n",
      "Uploaded data/processed_data\\upload_train_data\\sku04.csv, 4 files out of an estimated total of 5\n",
      "Uploading data/processed_data\\upload_train_data\\sku05.csv\n",
      "Uploaded data/processed_data\\upload_train_data\\sku05.csv, 5 files out of an estimated total of 5\n",
      "Uploaded 5 files\n",
      "Uploading an estimated of 5 files\n",
      "Uploading data/processed_data\\upload_inference_data\\sku01.csv\n",
      "Uploaded data/processed_data\\upload_inference_data\\sku01.csv, 1 files out of an estimated total of 5\n",
      "Uploading data/processed_data\\upload_inference_data\\sku02.csv\n",
      "Uploaded data/processed_data\\upload_inference_data\\sku02.csv, 2 files out of an estimated total of 5\n",
      "Uploading data/processed_data\\upload_inference_data\\sku03.csv\n",
      "Uploaded data/processed_data\\upload_inference_data\\sku03.csv, 3 files out of an estimated total of 5\n",
      "Uploading data/processed_data\\upload_inference_data\\sku04.csv\n",
      "Uploaded data/processed_data\\upload_inference_data\\sku04.csv, 4 files out of an estimated total of 5\n",
      "Uploading data/processed_data\\upload_inference_data\\sku05.csv\n",
      "Uploaded data/processed_data\\upload_inference_data\\sku05.csv, 5 files out of an estimated total of 5\n",
      "Uploaded 5 files\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_2a8558320b064331a817296439e1f39a"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# Connect to default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Upload train data\n",
    "ds_train_path = target_path + '_train'\n",
    "datastore.upload(src_dir=train_path, target_path=ds_train_path, overwrite=True)\n",
    "\n",
    "# Upload inference data\n",
    "ds_inference_path = target_path + '_inference'\n",
    "datastore.upload(src_dir=inference_path, target_path=ds_inference_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Register dataset in AML Workspace\n",
    "\n",
    "The last step is creating and registering [datasets](https://docs.microsoft.com/azure/machine-learning/concept-data#datasets) in Azure Machine Learning for the train and inference sets.\n",
    "\n",
    "Using a [FileDataset](https://docs.microsoft.com/python/api/azureml-core/azureml.data.file_dataset.filedataset) is currently the best way to take advantage of the many models pattern, so we create FileDatasets in the next cell. We then [register](https://docs.microsoft.com/azure/machine-learning/how-to-create-register-datasets#register-datasets) the FileDatasets in your Workspace; this associates the train/inference sets with simple names that can be easily referred to later on when we train models and produce forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'data/processed_data_inference')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"e554bcd7-12e1-45e2-9122-8a5f36687028\",\n",
       "    \"name\": \"data/processed_data_inference\",\n",
       "    \"version\": 1,\n",
       "    \"workspace\": \"Workspace.create(name='mmaml', subscription_id='051aa254-957d-4431-a6df-6caa8963bdd7', resource_group='manymodelsfatos')\"\n",
       "  }\n",
       "}"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "# Create file datasets\n",
    "ds_train = Dataset.File.from_files(path=datastore.path(ds_train_path), validate=False)\n",
    "ds_inference = Dataset.File.from_files(path=datastore.path(ds_inference_path), validate=False)\n",
    "\n",
    "# Register the file datasets\n",
    "dataset_name = target_path\n",
    "train_dataset_name = dataset_name + '_train'\n",
    "inference_dataset_name = dataset_name + '_inference'\n",
    "ds_train.register(ws, train_dataset_name, create_new_version=True)\n",
    "ds_inference.register(ws, inference_dataset_name, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have created your datasets, you are ready to move to one of the training notebooks to train and score the models:\n",
    "\n",
    "- Automated ML: please open [02_AutoML_Training_Pipeline.ipynb](Automated_ML/02_AutoML_Training_Pipeline/02_AutoML_Training_Pipeline.ipynb).\n",
    "- Custom Script: please open [02_CustomScript_Training_Pipeline.ipynb](Custom_Script/02_CustomScript_Training_Pipeline.ipynb)."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}